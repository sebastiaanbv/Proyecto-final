{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sebastiaanbv/Proyecto-final/blob/main/Proyeccion_peso_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Uqc4nOD2BSm"
      },
      "source": [
        "Paso 1: Importar librerias necesarias"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n",
        "!pip install xgboost\n",
        "!pip install dcor\n",
        "!pip install sweetviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bxCIxwbi5Q2",
        "outputId": "a136cc29-a5f9-422a-e5c3-437cbf91e400"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n",
            "Collecting dcor\n",
            "  Downloading dcor-0.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dcor) (1.26.4)\n",
            "Requirement already satisfied: numba>=0.51 in /usr/local/lib/python3.10/dist-packages (from dcor) (0.60.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dcor) (1.13.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from dcor) (1.4.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51->dcor) (0.43.0)\n",
            "Downloading dcor-0.6-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dcor\n",
            "Successfully installed dcor-0.6\n",
            "Collecting sweetviz\n",
            "  Downloading sweetviz-2.3.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.43.0 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (4.66.6)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (1.13.1)\n",
            "Requirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (3.1.4)\n",
            "Requirement already satisfied: importlib-resources>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (6.4.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.1->sweetviz) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->sweetviz) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->sweetviz) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.3->sweetviz) (1.16.0)\n",
            "Downloading sweetviz-2.3.1-py3-none-any.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sweetviz\n",
            "Successfully installed sweetviz-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AUZI-7FC2BS1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split #Me permite separar el df en test and training.\n",
        "from sklearn.linear_model import SGDRegressor, SGDClassifier #Se genera los gradientes. SGDRegressor = Rg Lineal y SGDClassifier = Rg Logistica\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder #Normaliza variables numericas\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, mean_absolute_error, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "#from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "#from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
        "import xgboost as xgb\n",
        "from xgboost import plot_importance, XGBRegressor, XGBClassifier\n",
        "#from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "#from xgboost import XGBClassifier\n",
        "#import matplotlib.pyplot as plt\n",
        "from sklearn.feature_selection import SelectKBest, f_regression, f_classif\n",
        "from sklearn import metrics\n",
        "import psycopg2\n",
        "import numpy as np\n",
        "import matplotlib.patches    as mpatches\n",
        "import sweetviz as sv\n",
        "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
        "from sklearn.feature_selection import mutual_info_regression, RFE, SelectKBest, f_classif, chi2, mutual_info_classif\n",
        "from dcor import distance_correlation\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import median_absolute_error\n",
        "from sklearn.model_selection import KFold, cross_validate\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import make_scorer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU1Rk4F92BS9"
      },
      "source": [
        "Paso 2: Cargar el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hNq8zlWm2BS_",
        "outputId": "55920716-bf30-4582-88e1-b330e51fe01d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/datos_granja_ok 1.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-fb624f197405>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Leer el dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0marchivo_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/datos_granja_ok 1.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchivo_csv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/datos_granja_ok 1.csv'"
          ]
        }
      ],
      "source": [
        "#Leer el dataset\n",
        "archivo_csv = '/content/datos_granja_ok 1.csv'\n",
        "df = pd.read_csv(archivo_csv,sep=';')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WfN3lN__KMEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FVhvtHCO2GZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZP1JIpX2BTD"
      },
      "source": [
        "Paso 3: EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Orw_nb5S2BTE"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['TipoPollo', 'Ventilacion', 'Genetica', 'Conversion'])\n",
        "df"
      ],
      "metadata": {
        "id": "_jJ5oFNXOKht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzoCt6UU2BTF"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L30rbCGo2BTH"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HnPGR_l2BTI"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k19sVVil2BTJ"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3Z-gPIY2BTK"
      },
      "outputs": [],
      "source": [
        "df.isnull().value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nlegp4a22BTL"
      },
      "outputs": [],
      "source": [
        "print('Recuento de columnas por tipo: ', df.dtypes.value_counts())\n",
        "print('Sumatoria de valores nulos en el dataframe: ', df.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnDE7ejr2BTM"
      },
      "outputs": [],
      "source": [
        "# Quitar espacios en blanco en todas las columnas de tipo string\n",
        "df[['Depto', 'Ciudad','TipoGranja','Sexo']] = df[['Depto', 'Ciudad','TipoGranja','Sexo']].apply(lambda x: x.str.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rZiEBTX2BTM"
      },
      "outputs": [],
      "source": [
        "df['Temperatura'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRxZN5jq2BTN"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,10))\n",
        "df['Temperatura'].value_counts()[:15].plot(kind='barh')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrGV2Uo62BTO"
      },
      "outputs": [],
      "source": [
        "df['Depto'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUGX4hap2BTP"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "df['Depto'].value_counts()[:15].plot(kind='pie')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMMmyQIR2BTQ"
      },
      "outputs": [],
      "source": [
        "print('la media es: ',df['Consumo'].mean())\n",
        "print('la mediana es: ',df['Consumo'].median())\n",
        "print('el valor mínimo de consumo es: : ',df['Consumo'].min())\n",
        "print('el valor máximo de consumo es: : ',df['Consumo'].max())\n",
        "print('la desviación éstandar de consumo es: : ',df['Consumo'].std())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTeQWhAH2BTS"
      },
      "outputs": [],
      "source": [
        "sns.histplot(data = df,x = 'Consumo')\n",
        "plt.axvline(x=df['Consumo'].mean(),color='red',linestyle='dashed',linewidth=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWEW8LMO2BTT"
      },
      "outputs": [],
      "source": [
        "print('la media de la edad es: ',df['Edad'].mean())\n",
        "print('la mediana de la edad es: ',df['Edad'].median())\n",
        "print('la desviación éstandar de la edad es: : ',df['Edad'].std())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP6iFV7q2BTU"
      },
      "outputs": [],
      "source": [
        "sns.histplot(data = df,x = 'Edad')\n",
        "plt.axvline(x=df['Edad'].mean(),color='red',linestyle='dashed',linewidth=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_WR4uhZ2BTU"
      },
      "outputs": [],
      "source": [
        "df['Sexo'].value_counts()\n",
        "plt.figure(figsize=(10,5))\n",
        "df['Sexo'].value_counts()[:15].plot(kind='pie')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fzo4D2-n2BTV"
      },
      "outputs": [],
      "source": [
        "df['TipoGranja'].value_counts()\n",
        "plt.figure(figsize=(10,5))\n",
        "df['TipoGranja'].value_counts()[:15].plot(kind='pie')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCNOy5672BTV"
      },
      "outputs": [],
      "source": [
        "print('la media del peso es: ',df['Peso'].mean())\n",
        "print('la mediana del peso es: ',df['Peso'].median())\n",
        "print('el valor mínimo del peso  es: : ',df['Peso'].min())\n",
        "print('el valor máximo del peso  es: : ',df['Peso'].max())\n",
        "print('la desviación éstandar del peso es: : ',df['Peso'].std())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LKQQD_I2BTW"
      },
      "source": [
        "La función sweet_report = sv.analyze(df) pertenece a la biblioteca Sweetviz de Python, diseñada para la exploración y visualización de datos.\n",
        "Al ejecutar sv.analyze(df), Sweetviz genera un informe exploratorio completo del DataFrame (df); para ver sus capacidades interactivas se genera en formato HTML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRh9rYk52BTX"
      },
      "outputs": [],
      "source": [
        "sweet_report = sv.analyze(df)\n",
        "sweet_report.show_html('C:\\Alex\\EAN ML\\Machine Learning\\Proyecto Final\\Datos\\sw_report_datos_granja_ok.csv_.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3RerdtK2BTX"
      },
      "source": [
        "Sacamos correlaciones con diferentes funciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pdc-p7822BTY"
      },
      "outputs": [],
      "source": [
        "#Estas funciones nos permite ver la correlacion de 2 variables con diferentes funciones, esta fucnjon recibe un datset y 2 dminesiones como parametros y retorna los coeficientes de correlacion entre ellas; explicadas por\n",
        "#Coeficiente de Corelacion que mide la fuerza y direccion de la relacion lineal. Si es 1 Indica correlacion positiva perfecta, -1 Indica correlacion negativa perfecta y 0 no hay correlacion.\n",
        "#PValor que indica la significancia estadistica de la correlacion encontrada. Si Pvalor >0.05 la correlacion es significativa.\n",
        "\n",
        "# Pearson Correlation\n",
        "def pearson_correlation(df, col1, col2):\n",
        "    corr, _ = pearsonr(df[col1], df[col2])\n",
        "    return corr\n",
        "\n",
        "# Spearman Correlation\n",
        "def spearman_correlation(df, col1, col2):\n",
        "    corr, _ = spearmanr(df[col1], df[col2])\n",
        "    return corr\n",
        "\n",
        "# Kendall Tau Correlation\n",
        "def kendall_correlation(df, col1, col2):\n",
        "    corr, _ = kendalltau(df[col1], df[col2])\n",
        "    return corr\n",
        "\n",
        "# Mutual Information (works for numerical features)\n",
        "def mutual_information(df, target_col, feature_col):\n",
        "    mi = mutual_info_regression(df[[feature_col]], df[target_col])\n",
        "    return mi[0]\n",
        "\n",
        "# Gráfico de Información Mutua entre variables numéricas y el target\n",
        "def plot_mutual_information(df, target_col):\n",
        "    numerical_df = filter_numerical_columns(df)\n",
        "    mi_scores = {col: mutual_information(df, target_col, col) for col in numerical_df.columns if col != target_col}\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(mi_scores.keys(), mi_scores.values())\n",
        "    plt.title(f\"Mutual Information with Target: {target_col}\")\n",
        "    plt.xlabel(\"Variables\")\n",
        "    plt.ylabel(\"Mutual Information Score\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjY2G0SA2BTZ"
      },
      "outputs": [],
      "source": [
        "# Pearson Correlation Heatmap for numerical columns\n",
        "def pearson_correlation_numerical(df,metodo):\n",
        "    # Filter numerical columns\n",
        "    numerical_df = filter_numerical_columns(df)\n",
        "\n",
        "    # Compute Pearson correlation matrix\n",
        "    corr = numerical_df.corr(method=metodo)\n",
        "\n",
        "    # Plot the heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr, annot=True, cmap='coolwarm', center=0, linewidths=0.5)\n",
        "    plt.title(f\"{metodo} Correlation Heatmap (Numerical Variables Only)\")\n",
        "    plt.show()\n",
        "    return corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeWjoN6k2BTZ"
      },
      "outputs": [],
      "source": [
        "def plot_scatter_with_regression(df, col1, col2):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.regplot(x=col1, y=col2, data=df, scatter_kws={\"s\": 50}, line_kws={\"color\": \"red\"})\n",
        "    plt.title(f'Scatter Plot with Regression Line: {col1} vs {col2}')\n",
        "    plt.xlabel(col1)\n",
        "    plt.ylabel(col2)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prykXA_n2BTa"
      },
      "outputs": [],
      "source": [
        "# Pearson correlation between x and y\n",
        "pearson_corr = pearson_correlation(df, 'Peso', 'Consumo')\n",
        "print(f\"Pearson Correlation: {pearson_corr}\")\n",
        "\n",
        "# Spearman correlation between x and y\n",
        "spearman_corr = spearman_correlation(df, 'Peso', 'Consumo')\n",
        "print(f\"Spearman Correlation: {spearman_corr}\")\n",
        "\n",
        "# Kendall correlation between x and y\n",
        "kendall_corr = kendall_correlation(df, 'Peso', 'Consumo')\n",
        "print(f\"Kendall Correlation: {kendall_corr}\")\n",
        "\n",
        "# Mutual Information between x and y (numerical only)\n",
        "mutual_info = mutual_information(df, 'Peso', 'Consumo')\n",
        "print(f\"Mutual Information: {mutual_info}\")\n",
        "\n",
        "# Filter only numerical variables in a DataFrame\n",
        "def filter_numerical_columns(df):\n",
        "    # Select only numerical columns (int, float)\n",
        "    return df.select_dtypes(include=[np.number])\n",
        "\n",
        "# Plot correlation heatmap\n",
        "pearson_correlation_numerical(df,\"spearman\")\n",
        "\n",
        "# Scatter plot with regression line between x and y\n",
        "plot_scatter_with_regression(df, 'Edad', 'Peso')\n",
        "\n",
        "plot_mutual_information(df, 'Peso')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-jejYtI2BTa"
      },
      "source": [
        "Paso 4: Normalización de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWjoLCNG2BTb"
      },
      "outputs": [],
      "source": [
        "#Este codigo elimina las columnas las cuales consideramos que no aportan al modelo por que no hay datos suficientes o tienen valor 0\n",
        "df = df.drop(columns=['Fecha','TipoAlimento','Depto','CO'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ck916vbJ2BTc"
      },
      "outputs": [],
      "source": [
        "# Codificar las variables independientes\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "df['Sexo'] = label_encoder.fit_transform(df['Sexo'])\n",
        "df['Ciudad'] = label_encoder.fit_transform(df['Ciudad'])\n",
        "df['TipoGranja'] = label_encoder.fit_transform(df['TipoGranja'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlHInBdX2BTd"
      },
      "outputs": [],
      "source": [
        "#En esta funcion queremos estandarizar las variables numericas; sin embargo y por caso de negocio consideramos que no es necesario.\n",
        "\n",
        "##MM_scaler=MinMaxScaler(feature_range=(-1,1))\n",
        "SD_scaler=StandardScaler()\n",
        "##X['Consumo']=MM_scaler.fit_transform(X['Consumo'].values.reshape(-1,1))\n",
        "##X['Edad']=MM_scaler.fit_transform(X['Edad'].values.reshape(-1,1))\n",
        "#y=SD_scaler.fit_transform(y.values.reshape(-1,1))\n",
        "#df['Consumo']=SD_scaler.fit_transform(df['Consumo'].values.reshape(-1,1))\n",
        "#df['Edad']=SD_scaler.fit_transform(df['Edad'].values.reshape(-1,1))\n",
        "#df['Unidades']=SD_scaler.fit_transform(df['Unidades'].values.reshape(-1,1))\n",
        "#df['Peso']=SD_scaler.fit_transform(df['Peso'].values.reshape(-1,1))\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REVISIÓN DE DATOS ATÍPICOS"
      ],
      "metadata": {
        "id": "O4mTUwLYmmjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " ##Crear la figura con subplots: 2 filas, 3 columnas\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12, 6))  # Ajusta figsize si es necesario\n",
        "\n",
        "# Primer gráfico: Boxplot de Temperatura\n",
        "sns.boxplot(ax=axes[0, 0], y=df['Temperatura'])\n",
        "axes[0, 0].set_title('Boxplot de temperatura')\n",
        "\n",
        "# Segundo gráfico: Boxplot de Consumo\n",
        "sns.boxplot(ax=axes[0, 1], y=df['Consumo'])\n",
        "axes[0, 1].set_title('Boxplot de consumo')\n",
        "\n",
        "# Tercer gráfico: Boxplot de Unidades\n",
        "sns.boxplot(ax=axes[0, 2], y=df['Unidades'])\n",
        "axes[0, 2].set_title('Boxplot de unidades')\n",
        "\n",
        "# Cuarto gráfico: Boxplot de Edad\n",
        "sns.boxplot(ax=axes[1, 0], y=df['Edad'])\n",
        "axes[1, 0].set_title('Boxplot de edad')\n",
        "\n",
        "# Quinto gráfico: Boxplot de Peso\n",
        "sns.boxplot(ax=axes[1, 1], y=df['Peso'])\n",
        "axes[1, 1].set_title('Boxplot de Peso')\n",
        "\n",
        "# Si hay otro gráfico, por ejemplo, puedes dejar el último espacio vacío o poner un gráfico adicional si es necesario\n",
        "# axes[1, 2] está vacío en este caso\n",
        "\n",
        "# Ajustar el layout para que no se solapen los gráficos\n",
        "plt.tight_layout()\n",
        "\n",
        "# Mostrar los gráficos\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6UJR4neBml-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evidenciamos datos atípicos en todas las variables numéricas, por lo que procedemos a eliminarlos.\n"
      ],
      "metadata": {
        "id": "b37H5b0nnQLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eliminar datos atípicos mediante z-score"
      ],
      "metadata": {
        "id": "crDFXNt0nZ-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "columnas = ['Temperatura', 'Consumo', 'Unidades', 'Edad', 'Peso']\n",
        "\n",
        "# Crear una copia del DataFrame original para almacenar los datos filtrados\n",
        "df_filtered = df.copy()\n",
        "\n",
        "# Calcular Z-Score para cada columna y eliminar outliers de manera acumulativa pero correcta\n",
        "for columna in columnas:\n",
        "    # Calcular el z-score de cada valor en la columna original\n",
        "    z_scores = stats.zscore(df_filtered[columna])\n",
        "    # Filtrar los datos para mantener solo aquellos con un z-score en el rango [-3, 3]\n",
        "    df_filtered = df_filtered[(z_scores > -3) & (z_scores < 3)]\n",
        "\n",
        "    # Imprimir cuántos outliers fueron eliminados para esta columna específica\n",
        "    outliers_count = len(df) - len(df_filtered)\n",
        "    print(f\"Valores atípicos eliminados en la columna {columna}: [{outliers_count} rows]\")\n",
        "\n",
        "# Mostrar el DataFrame resultante sin outliers\n",
        "print(df_filtered)\n",
        "\n"
      ],
      "metadata": {
        "id": "-4QH-UQ1m9fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " ##Crear la figura con subplots: 2 filas, 3 columnas\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12, 6))  # Ajusta figsize si es necesario\n",
        "\n",
        "# Primer gráfico: Boxplot de Temperatura\n",
        "sns.boxplot(ax=axes[0, 0], y=df_filtered['Temperatura'])\n",
        "axes[0, 0].set_title('Boxplot de temperatura')\n",
        "\n",
        "# Segundo gráfico: Boxplot de Consumo\n",
        "sns.boxplot(ax=axes[0, 1], y=df_filtered['Consumo'])\n",
        "axes[0, 1].set_title('Boxplot de consumo')\n",
        "\n",
        "# Tercer gráfico: Boxplot de Unidades\n",
        "sns.boxplot(ax=axes[0, 2], y=df_filtered['Unidades'])\n",
        "axes[0, 2].set_title('Boxplot de unidades')\n",
        "\n",
        "# Cuarto gráfico: Boxplot de Edad\n",
        "sns.boxplot(ax=axes[1, 0], y=df_filtered['Edad'])\n",
        "axes[1, 0].set_title('Boxplot de edad')\n",
        "\n",
        "# Quinto gráfico: Boxplot de Peso\n",
        "sns.boxplot(ax=axes[1, 1], y=df_filtered['Peso'])\n",
        "axes[1, 1].set_title('Boxplot de Peso')\n",
        "\n",
        "# Si hay otro gráfico, por ejemplo, puedes dejar el último espacio vacío o poner un gráfico adicional si es necesario\n",
        "# axes[1, 2] está vacío en este caso\n",
        "\n",
        "# Ajustar el layout para que no se solapen los gráficos\n",
        "plt.tight_layout()\n",
        "\n",
        "# Mostrar los gráficos\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1oKgheodoZVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wo4grRj2BTg"
      },
      "source": [
        "Una vez los datos normalizados podemos revisar nuevamente las matrices de correlacion mediante diferentes funciones. Pearson, Kendall y Spearman"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kY_sjyy12BTs"
      },
      "outputs": [],
      "source": [
        "# Análisis de correlación que depende del metodo seleccionado en la variable metodo, solo son  aplicables 3 tipos: pearson, spearman y kendall\n",
        "#Spearman: Adecuada para relaciones no lineales, es robusta frente a outliers\n",
        "#kendall: Adecuada para relaciones no lineales, es robusta frente a outliers\n",
        "#Pearson: Mide la relacion lineal entre 2 variables cuantitativas.\n",
        "metodo = \"spearman\"\n",
        "corr_matrix = df_filtered.corr(method=metodo)\n",
        "plt.figure(figsize=(15,10 ))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title(f'{metodo} Matriz de Correlación')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Análisis de correlación que depende del metodo seleccionado en la variable metodo, solo son  aplicables 3 tipos: pearson, spearman y kendall\n",
        "#Spearman: Adecuada para relaciones no lineales, es robusta frente a outliers\n",
        "#kendall: Adecuada para relaciones no lineales, es robusta frente a outliers\n",
        "#Pearson: Mide la relacion lineal entre 2 variables cuantitativas.\n",
        "metodo = \"spearman\"\n",
        "corr_matrix = df_filtered.corr(method=metodo)\n",
        "plt.figure(figsize=(15,10 ))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title(f'{metodo} Matriz de Correlación')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rAun3LNeCW_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M22VkTiF2BTs"
      },
      "outputs": [],
      "source": [
        "#Con esta funcion podemos determinar la fuerza en la relacion de las variables frente a nuestra variable objetivo que es el peso y depende de la funcion de correlacion que seleccionemos\n",
        "target_corr = corr_matrix[\"Peso\"].sort_values(ascending=False)\n",
        "print(target_corr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWyP7O2z2BTt"
      },
      "outputs": [],
      "source": [
        "# Mean Squared Error (MSE)\n",
        "def mean_squared_error_metric(df_filtered, col1, col2):\n",
        "    mse = mean_squared_error(df[col1], df[col2])\n",
        "    return mse\n",
        "\n",
        "# Mean Absolute Error (MAE)\n",
        "def mean_absolute_error_metric(df_filtered, col1, col2):\n",
        "    mae = mean_absolute_error(df[col1], df[col2])\n",
        "    return mae\n",
        "\n",
        "# R-squared (R²)\n",
        "def r_squared_metric(df_filtered, col1, col2):\n",
        "    r2 = r2_score(df[col1], df[col2])\n",
        "    return r2\n",
        "\n",
        "# Distance Correlation\n",
        "def distance_correlation_metric(df_filtered, col1, col2):\n",
        "    dcor = distance_correlation(df[col1], df[col2])\n",
        "    return dcor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5XsK3j52BTt"
      },
      "source": [
        "Distance Correlation (correlación de distancia) es una métrica que mide cualquier tipo de dependencia entre dos variables, ya sea lineal o no lineal.\n",
        "Es útil para detectar relaciones complejas entre variables, ya que no se limita a relaciones lineales como el coeficiente de Pearson.\n",
        "Interpretación del valor: 0 signifca que no tinen ninguna relacion estadistica y 1 indica dependencia total es decir hay una relación muy fuerte entre las variables.\n",
        "Es util cuando se sospecha que puede haber una relacion compleja o no lineal entre las variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xO2w2Dfc2BTu"
      },
      "outputs": [],
      "source": [
        "# Mean Squared Error (MSE) between x and y\n",
        "mse = mean_squared_error_metric(df_filtered, 'Peso', 'Edad')\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "# Mean Absolute Error (MAE) between x and y\n",
        "mae = mean_absolute_error_metric(df_filtered, 'Peso', 'Edad')\n",
        "print(f\"Mean Absolute Error: {mae}\")\n",
        "\n",
        "# R-squared (R²) between x and y\n",
        "r2 = r_squared_metric(df_filtered, 'Peso', 'Edad')\n",
        "print(f\"R-squared: {r2}\")\n",
        "\n",
        "# Distance Correlation between x and y\n",
        "dcor =distance_correlation_metric(df_filtered, 'Peso', 'Consumo')\n",
        "print(f\"Distance Correlation: {dcor}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX-w9-En2BTu"
      },
      "source": [
        "Paso 5: Seleccion de variables importantes que aportan al modelo ya sea para prediccion o regresion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnX3ZiQx2BTw"
      },
      "source": [
        "La función SelectKBest es una herramienta en la biblioteca Scikit-Learn que selecciona automáticamente las K mejores características de un conjunto de datos, basándose en una métrica de puntuación específica.\n",
        "Es útil para reducción de dimensionalidad y selección de características, permitiendo mejorar la eficiencia y precisión de los modelos al enfocarse en las variables más relevantes.\n",
        "Tenemos varias metricas de seleccion o puntuacion de variables: Chi1: para datos categoricos, f_clasiff para analisis de varianza util para clasificación, f_regression Prueba F util para regresion y mutual_info_classif ó mutual_info_regression util para irar dependencias de variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zRKSOmf2BTx"
      },
      "outputs": [],
      "source": [
        "# Definir las variables predictoras y la variable objetivo\n",
        "X = df_filtered.drop(columns=[\"Peso\"])  # \"Peso\" es la variable objetivo\n",
        "y = df_filtered[\"Peso\"]\n",
        "\n",
        "# Seleccionar las k mejores variables (por ejemplo, k=5)\n",
        "selector = SelectKBest(score_func=mutual_info_regression, k=5)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Ver cuáles variables fueron seleccionadas\n",
        "selected_features = X.columns[selector.get_support()]\n",
        "print(\"Las mejores variables:\", selected_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMXiCVX-2BTy"
      },
      "outputs": [],
      "source": [
        "# Aplicar Lasso con validación cruzada\n",
        "lasso = LassoCV(cv=5, random_state=1).fit(X, y)\n",
        "# Seleccionar las variables con coeficientes distintos de cero\n",
        "selected_features = X.columns[lasso.coef_ != 0]\n",
        "print(\"Las mejores variables seleccionadas por Lasso:\", selected_features)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Evaluar el modelo en el conjunto de prueba\n",
        "y_pred = lasso.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2=r2_score(y_test, y_pred)\n",
        "print(\"Error cuadrático medio en test:\", mse)\n",
        "print(\"R2 :\", r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxRwe_QF2BTz"
      },
      "outputs": [],
      "source": [
        "# Ajustar un modelo de bosque aleatorio\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=5)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Ver la importancia de las variables\n",
        "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "importances = importances.sort_values(ascending=False)\n",
        "print(\"Importancia de las variables:\", importances)\n",
        "\n",
        "# Seleccionar las variables más importantes (por ejemplo, con importancia > 0.1)\n",
        "selected_features = importances[importances > 0.1].index\n",
        "print(\"Las mejores variables seleccionadas por Random Forest:\", selected_features)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Evaluar el modelo en el conjunto de prueba\n",
        "y_pred = rf.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2=r2_score(y_test, y_pred)\n",
        "print(\"Error cuadrático medio en test:\", mse)\n",
        "print(\"R2 :\", r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUvuBPQ42BT0"
      },
      "source": [
        "La función RFE(estimator=modelo_base, n_features_to_select=5) en Scikit-Learn se utiliza para realizar Eliminación Recursiva de Características, un método de selección de características.\n",
        "Con RFE, el objetivo es seleccionar el subconjunto de variables más relevante para el modelo, mejorando la eficiencia y, en algunos casos, la precisión.\n",
        "Dentro de los modelos base podemos tener: linearRegression, RandomForestRegressor o SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0kNfTQ82BT1"
      },
      "outputs": [],
      "source": [
        "# Definir el modelo base y aplicar RFE\n",
        "modelo_base = LinearRegression()\n",
        "rfe = RFE(estimator=modelo_base, n_features_to_select=5)\n",
        "X_rfe = rfe.fit_transform(X, y)\n",
        "\n",
        "# Obtener las variables seleccionadas\n",
        "selected_features = X.columns[rfe.support_]\n",
        "print(\"Las mejores variables seleccionadas por RFE:\", selected_features)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Evaluar el modelo en el conjunto de prueba\n",
        "y_pred = rfe.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2=r2_score(y_test, y_pred)\n",
        "print(\"Error cuadrático medio en test:\", mse)\n",
        "print(\"R2 :\", r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Pp9XsKD2BT2"
      },
      "outputs": [],
      "source": [
        "# Suponiendo que `X` contiene las variables predictoras y `y` la variable objetivo\n",
        "# Define y entrena el modelo de XGBoost\n",
        "modelo_xgb = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1)\n",
        "modelo_xgb.fit(X, y)\n",
        "\n",
        "# Obtener la importancia de cada variable\n",
        "importancia = modelo_xgb.feature_importances_\n",
        "\n",
        "# Convertir la importancia en un DataFrame para ordenar y visualizar\n",
        "importancia_df = pd.DataFrame({\n",
        "    'Variable': X.columns,\n",
        "    'Importancia': importancia\n",
        "}).sort_values(by='Importancia', ascending=False)\n",
        "\n",
        "print(\"Importancia de las variables:\")\n",
        "print(importancia_df)\n",
        "\n",
        "# Graficar la importancia de variables\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_importance(modelo_xgb, max_num_features=10)  # Puedes ajustar el número de variables a mostrar\n",
        "plt.title(\"Importancia de las Variables según XGBoost\")\n",
        "plt.show()\n",
        "\n",
        "pred = modelo_xgb.predict(X_test)\n",
        "\n",
        "print(\"R^2:\",metrics.r2_score(y_test, pred))\n",
        "print(\"Adjusted R^2:\",1 - (1-metrics.r2_score(y_test, pred))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))\n",
        "print(\"MAE:\",metrics.mean_absolute_error(y_test, pred))\n",
        "print(\"MSE:\",metrics.mean_squared_error(y_test, pred))\n",
        "print(\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test, pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17wCz1KR2BT3"
      },
      "outputs": [],
      "source": [
        "top_k = 5\n",
        "mejores_variables = importancia_df.head(top_k)['Variable'].values\n",
        "print(\"Las mejores variables seleccionadas por XGBoost:\", mejores_variables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WkbCAFA2BT5"
      },
      "source": [
        "El VIF (Variance Inflation Factor o Factor de Inflación de Varianza) es una métrica usada en machine learning y estadística para medir la multicolinealidad entre las variables predictoras de un modelo, especialmente en regresión lineal. La multicolinealidad ocurre cuando dos o más variables independientes están altamente correlacionadas, lo que puede hacer que los coeficientes de un modelo sean inestables y difíciles de interpretar.\n",
        "\n",
        "Interpretación de los valores de VIF\n",
        "VIF = 1: La variable no tiene correlación con otras variables, lo que es ideal.\n",
        "1 < VIF < 5: Existe una correlación moderada con otras variables, lo que generalmente es aceptable.\n",
        "VIF > 5: La multicolinealidad es significativa; debes investigar si puedes reducir la correlación.\n",
        "VIF > 10: Muy alta multicolinealidad. Generalmente, es una señal de que una o más variables deben eliminarse o transformarse para reducir la correlación.\n",
        "El VIF es, por tanto, una herramienta importante en la selección de características para crear modelos más estables y eficientes en machine learning, especialmente en modelos lineales o donde se requiere una interpretación clara de los coeficientes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wa0v2mCR2BT5"
      },
      "outputs": [],
      "source": [
        "# Supongamos que 'df' es tu DataFrame con variables independientes\n",
        "#X = df[['Edad','Humedad','consumoAgua','Temperatura','PesoGr']]  # Selecciona tus variables independientes\n",
        "#X = df[['Temperatura','Consumo','PesoGr','Unidades','Edad','Sexo', 'TipoGranja','Humedad','HorasLuz','CalidadCama','consumoAgua']]  # Selecciona tus variables independientes\n",
        "# Calcula el VIF para cada variable\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data['Variable'] = X.columns\n",
        "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "print(vif_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xobqWggD2BT6"
      },
      "outputs": [],
      "source": [
        "#PCA para la importancia de características con 3 componentes principales\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "pca = PCA(n_components=3)  # Limitar a 3 componentes principales #USANDO 3 COMPONENTES PRINCIPALES.\n",
        "X_pca = pca.fit_transform(X_train)\n",
        "\n",
        "# Importancia de características por componente principal\n",
        "pca_components = pd.DataFrame(pca.components_, columns=X_train.columns)\n",
        "pca_components.index = [f'Componente {i+1}' for i in range(len(pca_components))]\n",
        "\n",
        "# Gráfico de Importancia de Características por Componente Principal\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(pca_components, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Importancia de Características por Componente Principal (PCA)')\n",
        "plt.show()\n",
        "\n",
        "# Visualizar contribuciones de las características a los tres componentes principales\n",
        "plt.figure(figsize=(12, 8))\n",
        "pca_components.T.plot(kind='bar', figsize=(10, 8))\n",
        "plt.title('Contribuciones de Características a los Tres Primeros Componentes Principales')\n",
        "plt.xlabel('Características')\n",
        "plt.ylabel('Contribución')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SZgnR7m2BT7"
      },
      "source": [
        "Paso 6: Seleccion de las mejores variables para prediccion y modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8axjtzIj2BT7"
      },
      "outputs": [],
      "source": [
        "X = df_filtered[['Temperatura', 'Edad', 'Sexo', 'TipoGranja', 'Consumo']] # \"Peso\" es la variable objetivo\n",
        "y = df_filtered[\"Peso\"]\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Espacio de hiperparámetros para RandomForest\n",
        "rf_param_grid = {\n",
        "    'n_estimators': np.arange(50, 200, 10),\n",
        "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Espacio de hiperparámetros para XGBoost\n",
        "xgb_param_grid = {\n",
        "    'n_estimators': np.arange(50, 200, 10),\n",
        "    'max_depth': np.arange(3, 10),\n",
        "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    'gamma': [0, 0.1, 0.2, 0.3],\n",
        "    'reg_alpha': [0, 0.1, 0.5, 1],\n",
        "    'reg_lambda': [1, 1.5, 2]\n",
        "}\n",
        "\n",
        "# Inicializar modelos\n",
        "rf_model = RandomForestRegressor()\n",
        "xgb_model = XGBRegressor(objective='reg:squarederror')\n",
        "\n",
        "# Configuración de RandomizedSearchCV para RandomForest\n",
        "rf_random_search = RandomizedSearchCV(\n",
        "    estimator=rf_model,\n",
        "    param_distributions=rf_param_grid,\n",
        "    n_iter=50,\n",
        "    scoring='r2',\n",
        "    cv=5,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Configuración de RandomizedSearchCV para XGBoost\n",
        "xgb_random_search = RandomizedSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_distributions=xgb_param_grid,\n",
        "    n_iter=50,\n",
        "    scoring='r2',\n",
        "    cv=5,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Ajustar el modelo RandomForest con RandomizedSearchCV\n",
        "rf_random_search.fit(X_train, y_train)\n",
        "print(\"Mejores parámetros para RandomForest:\", rf_random_search.best_params_)\n",
        "print(\"Mejor puntuación R² para RandomForest:\", rf_random_search.best_score_)\n",
        "\n",
        "# Ajustar el modelo XGBoost con RandomizedSearchCV\n",
        "xgb_random_search.fit(X_train, y_train)\n",
        "print(\"Mejores parámetros para XGBoost:\", xgb_random_search.best_params_)\n",
        "print(\"Mejor puntuación R² para XGBoost:\", xgb_random_search.best_score_)\n"
      ],
      "metadata": {
        "id": "K8phbBH_nQN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dividir el conjunto de datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalizar los datos\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Lista de modelos a evaluar\n",
        "# Definir los mejores parámetros para RandomForest\n",
        "best_rf_params = {\n",
        "    'n_estimators': 100,\n",
        "    'min_samples_split': 2,\n",
        "    'min_samples_leaf': 1,\n",
        "    'max_features': 'sqrt',\n",
        "    'max_depth': None,\n",
        "    'bootstrap': False\n",
        "}\n",
        "\n",
        "# Definir los mejores parámetros para XGBoost\n",
        "best_xgb_params = {\n",
        "    'subsample': 0.8,\n",
        "    'reg_lambda': 2,\n",
        "    'reg_alpha': 0,\n",
        "    'n_estimators': 180,\n",
        "    'max_depth': 8,\n",
        "    'learning_rate': 0.1,\n",
        "    'gamma': 0,\n",
        "    'colsample_bytree': 0.8\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Inicializar modelos con los mejores parámetros ajustados para evitar overfitting\n",
        "rf_model_best = RandomForestRegressor(**best_rf_params)\n",
        "xgb_model_best = XGBRegressor(objective='reg:squarederror', **best_xgb_params)\n",
        "\n",
        "# Ajustar el modelo RandomForest a los datos normalizados\n",
        "rf_model_best.fit(X_train, y_train)\n",
        "print(\"Modelo RandomForest ajustado con los mejores parámetros regularizados.\")\n",
        "\n",
        "# Ajustar el modelo XGBoost a los datos normalizados\n",
        "xgb_model_best.fit(X_train, y_train)\n",
        "print(\"Modelo XGBoost ajustado con los mejores parámetros regularizados.\")\n",
        "\n",
        "# Evaluar los modelos en el conjunto de prueba\n",
        "models = {'RandomForest': rf_model_best, 'XGBoost': xgb_model_best}\n",
        "\n",
        "# Mostrar las métricas de entrenamiento y prueba para cada modelo\n",
        "for name, model in models.items():\n",
        "    # Predecir en el conjunto de entrenamiento y prueba\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # Calcular métricas\n",
        "    train_MAE = mean_absolute_error(y_train, y_train_pred)\n",
        "    test_MAE = mean_absolute_error(y_test, y_test_pred)\n",
        "    train_MSE = mean_squared_error(y_train, y_train_pred)\n",
        "    test_MSE = mean_squared_error(y_test, y_test_pred)\n",
        "    train_R2 = r2_score(y_train, y_train_pred)\n",
        "    test_R2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "    # Imprimir las métricas\n",
        "    print(f\"\\n{name} Model Metrics:\")\n",
        "    print(f\"  Train MAE: {train_MAE:.4f}\")\n",
        "    print(f\"  Test MAE: {test_MAE:.4f}\")\n",
        "    print(f\"  Train MSE: {train_MSE:.4f}\")\n",
        "    print(f\"  Test MSE: {test_MSE:.4f}\")\n",
        "    print(f\"  Train R2: {train_R2:.4f}\")\n",
        "    print(f\"  Test R2: {test_R2:.4f}\")\n"
      ],
      "metadata": {
        "id": "oKHeWH3OMszS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de modelos a evaluar\n",
        "models = {\n",
        "\n",
        "    'Random Forest': RandomForestRegressor(**best_rf_params),\n",
        "    'XGBoost': XGBRegressor(objective='reg:squarederror', **best_xgb_params),\n",
        "}\n",
        "\n",
        "# Configuración de KFold\n",
        "kf = KFold(n_splits=7, shuffle=True, random_state=42)\n",
        "\n",
        "# Función para calcular R² ajustado\n",
        "def adjusted_r2(r2, n, p):\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "# Métricas personalizadas\n",
        "scoring = {\n",
        "    'MAE': make_scorer(mean_absolute_error),\n",
        "    'MSE': make_scorer(mean_squared_error),\n",
        "    'R2': make_scorer(r2_score),\n",
        "}\n",
        "\n",
        "# Número de muestras y características\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "# Evaluación de modelos\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nEvaluando {name}\")\n",
        "\n",
        "    results = cross_validate(\n",
        "        model,\n",
        "        X,\n",
        "        y,\n",
        "        cv=kf,\n",
        "        scoring=scoring,\n",
        "        return_train_score=True\n",
        "    )\n",
        "\n",
        "    # Cálculo de R² ajustado para cada división\n",
        "    train_r2_adj = [adjusted_r2(r2, n_samples, n_features) for r2 in results['train_R2']]\n",
        "    test_r2_adj = [adjusted_r2(r2, n_samples, n_features) for r2 in results['test_R2']]\n",
        "\n",
        "    # Mostrar resultados promedio\n",
        "    print(\"Métricas de prueba\")\n",
        "    print(f\"MAE: {np.mean(results['test_MAE'])}\")\n",
        "    print(f\"MSE: {np.mean(results['test_MSE'])}\")\n",
        "    print(f\"R2: {np.mean(results['test_R2'])}\")\n",
        "    print(f\"R2 Ajustado: {np.mean(test_r2_adj)}\")\n",
        "\n",
        "    print(\"\\nMétricas de entrenamiento\")\n",
        "    print(f\"MAE: {np.mean(results['train_MAE'])}\")\n",
        "    print(f\"MSE: {np.mean(results['train_MSE'])}\")\n",
        "    print(f\"R2: {np.mean(results['train_R2'])}\")\n",
        "    print(f\"R2 Ajustado: {np.mean(train_r2_adj)}\")"
      ],
      "metadata": {
        "id": "6z6eCDOd4Gyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graficar las métricas de entrenamiento y prueba en la misma gráfica para cada partición\n",
        "fig, axes = plt.subplots(4, 1, figsize=(10, 16), sharex=True)\n",
        "fig.suptitle(\"Métricas de Entrenamiento y Prueba por Partición\")\n",
        "\n",
        "# Graficar MAE\n",
        "axes[0].plot(results['train_MAE'], marker='o', linestyle='-', label='Train MAE')\n",
        "axes[0].plot(results['test_MAE'], marker='o', linestyle='--', label='Test MAE')\n",
        "axes[0].set_ylabel(\"MAE\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Graficar MSE\n",
        "axes[1].plot(results['train_MSE'], marker='o', linestyle='-', label='Train MSE')\n",
        "axes[1].plot(results['test_MSE'], marker='o', linestyle='--', label='Test MSE')\n",
        "axes[1].set_ylabel(\"MSE\")\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "# Graficar R2\n",
        "axes[2].plot(results['train_R2'], marker='o', linestyle='-', label='Train R2')\n",
        "axes[2].plot(results['test_R2'], marker='o', linestyle='--', label='Test R2')\n",
        "axes[2].set_ylabel(\"R2\")\n",
        "axes[2].legend()\n",
        "axes[2].grid(True)\n",
        "\n",
        "# Graficar R2 ajustado\n",
        "axes[3].plot(train_r2_adj, marker='o', linestyle='-', label='Train R2 Ajustado')\n",
        "axes[3].plot(test_r2_adj, marker='o', linestyle='--', label='Test R2 Ajustado')\n",
        "axes[3].set_ylabel(\"R2 Ajustado\")\n",
        "axes[3].legend()\n",
        "axes[3].grid(True)\n",
        "\n",
        "axes[-1].set_xlabel(\"Partición\")\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mu7FwsRiMGMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Temperatura',  'Edad', 'Sexo', 'TipoGranja','Consumo'\n",
        "# Nuevos datos para predicción\n",
        "print(X)\n",
        "\n",
        "\n",
        "  # Nuevos datos para predicción\n",
        "nuevos_datos = pd.DataFrame({\n",
        "\n",
        "    'Temperatura':[31],\n",
        "    'Edad':[31],\n",
        "    'Sexo':[1],\n",
        "    'TipoGranja':[1],\n",
        "    'Consumo':[586]\n",
        "})\n",
        "\n",
        "# Realizar predicciones\n",
        "prediccion_peso = xgb_model_best.predict(nuevos_datos)\n",
        "print(\"El peso seria de:\", prediccion_peso[0])"
      ],
      "metadata": {
        "id": "b3z3_BVrRUDz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "EAN",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}